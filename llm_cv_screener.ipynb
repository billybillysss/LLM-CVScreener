{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter  import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 24.65it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/cv\"\n",
    "persist_directory= \"./vectorstor/db\"\n",
    "n_doc = 3 ##number of documents return\n",
    "\n",
    "loader = DirectoryLoader(path = path, show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "embeddings = OllamaEmbeddings(base_url=os.getenv(\"OLLAMA_ADDR\"), model=os.getenv(\"OLLAMA_MODEL\"))\n",
    "\n",
    "llm = Ollama(base_url=os.getenv(\"OLLAMA_ADDR\"), model=os.getenv(\"OLLAMA_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "text_splitter.split_documents(documents)\n",
    "vectorstore = Chroma.from_documents(persist_directory=persist_directory, embedding=embeddings, documents=documents)\n",
    "retriver = vectorstore.as_retriever(search_type=\"mmr\",search_kwargs= {\"k\": n_doc} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reader(path):\n",
    "    with open(path, mode=\"r+\", encoding= \"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responsibilities:\n",
      "• Design, develop, and maintain multiple databases within the data warehouse. \n",
      "• Create, implement, and test data models and database management systems.\n",
      "• Conduct research and support internal/external groups with the selection and implementation of applications and database management tools.\n",
      "• Identify and utilize database management systems to aggregate and analyze data. \n",
      "• In collaboration with the VP, IBT and the Senior Data Warehouse Engineer, develop and implement data administration policies, standards, and models. \n",
      "• Work with internal departments and colleagues to understand and document specific data and user requirements, data collection and administration policies, and data access rules. \n",
      "• Develop and implement procedures for determining network access and usage and for the backup and recovery of data. \n",
      "• Write scripts to support automation, data extraction, and reporting.\n",
      "• Design and automate routine and self-service reporting solutions (i.e., Power BI).\n",
      "• Identify opportunities and provide recommendations to enhance efficiencies in data management and analysis. \n",
      "• Create and maintain system context diagrams and process maps, capturing flows of information between systems and typical customer flows.\n",
      "• Analyze business operations and workflows to understand and define opportunities where information solutions can be implemented to mitigate business challenges.\n",
      "• Perform ad-hoc data extraction, analysis, and interpretation.\n"
     ]
    }
   ],
   "source": [
    "jd_path = \"./data/jd/job_description.txt\"\n",
    "jd=file_reader(jd_path)\n",
    "print(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 10, updating n_results = 10\n"
     ]
    }
   ],
   "source": [
    "screen_result = retriver.invoke(F\"\"\"find the CV with the best match with the JD as below:\n",
    "   {jd}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Name: Sarah Miller\\n\\nEmail: sarah.miller@example.com\\n\\nPhone: +1-555-678-9012\\n\\nLocation: Edmonton, AB\\n\\nProfessional Summary\\n\\nDetail-oriented data analyst with 4 years of experience in data analysis, visualization,\\n\\nand reporting. Expertise in extracting insights from complex datasets to support\\n\\nstrategic business decisions.\\n\\nSkills\\n\\nData Analysis (Python, R)\\n\\nData Visualization (Tableau, Power BI)\\n\\nSQL, Excel\\n\\nStatistical Analysis\\n\\nData Cleaning and Preparation • Business Intelligence Tools\\n\\nWork Experience\\n\\nData Analyst\\n\\nMarket Insights Inc., Edmonton, AB\\n\\nFebruary 2020 - Present\\n\\nAnalyzed market trends and customer behavior to inform marketing strategies.\\n\\nCreated interactive dashboards and visualizations to present data insights.\\n\\nConducted statistical analyses to support product development decisions.\\n\\nJunior Data Analyst\\n\\nInsight Analytics, Edmonton, AB\\n\\nJuly 2018 - January 2020\\n\\nAssisted in data cleaning and preparation for analysis.\\n\\nGenerated reports and visualizations to support business operations.\\n\\nCollaborated with cross-functional teams to gather data requirements.\\n\\nEducation\\n\\nBachelor of Science in Statistics\\n\\nUniversity of Alberta, Edmonton, AB\\n\\n2014 - 2018\\n\\nCertifications\\n\\nCertified Business Intelligence Professional (CBIP)', metadata={'source': 'data\\\\cv\\\\Sarah_Mille_CV.pdf'}),\n",
       " Document(page_content='Name: Sophia Kim Email: sophia.kim@example.com Phone: +1-555-789-0123 Location: Calgary, AB\\n\\nProfessional Summary Innovative data scientist with a passion for uncovering hidden insights in large datasets. Over 3 years of experience in predictive modeling and data visualization.\\n\\nSkills\\n\\nl Python, R l Machine Learning l Data Visualization (Tableau, Power BI) l SQL l Statistical Analysis\\n\\nWork Experience\\n\\nData Scientist Insightful Analytics, Calgary, AB June 2021 - Present\\n\\nl Developed machine learning models to predict customer churn. l Created visualizations to present data insights to stakeholders.\\n\\nData Analyst Data Solutions, Calgary, AB September 2018 - May 2021\\n\\nl Conducted data analysis to support business decisions. l Prepared reports and dashboards for management.\\n\\nEducation Bachelor of Science in Statistics University of Calgary, Calgary, AB 2014 - 2018\\n\\nCertifications Certified Data Scientist (CDS) - Data Science Council of America', metadata={'source': 'data\\\\cv\\\\Sophia_Kim_CV.pdf'}),\n",
       " Document(page_content='Name: Daniel Williams Email: daniel.williams@example.com Phone: +1-555-012-3456 Location: Toronto, ON\\n\\nProfessional Summary Experienced data engineer with over 5 years of expertise in data pipeline development and cloud data solutions. Adept at ensuring data integrity and optimizing data workflows.\\n\\nSkills\\n\\nl SQL, NoSQL l ETL Processes l Cloud Platforms (AWS, Azure) l Python, Java l Data Warehousing\\n\\nWork Experience\\n\\nSenior Data Engineer Cloud Innovations, Toronto, ON October 2019 - Present\\n\\nl Designed and implemented scalable data pipelines on AWS. l Optimized ETL workflows for better performance.\\n\\nData Engineer Data Systems Ltd., Toronto, ON July 2015 - September 2019\\n\\nl Maintained and improved data integration processes.\\n\\nEducation Bachelor of Science in Information Technology Ryerson University, Toronto, ON 2011 - 2015\\n\\nCertifications AWS Certified Solutions Architect', metadata={'source': 'data\\\\cv\\\\Daniel_Williams_CV.pdf'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Chroma.from_documents(documents=screen_result, embedding=embeddings).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"Summarize and highlight the experience, certificate or education in the CV which match with the JD as below:\n",
    "\n",
    "CV:\n",
    "{context}\n",
    "\n",
    "JD:\n",
    "{job_description}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = llm\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever , \"job_description\": lambda x: jd}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "res =chain.invoke(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided CVs and JD, I've highlighted the relevant experience, certificate, or education that match with the responsibilities mentioned in the JD:\n",
      "\n",
      "**CV 1: Sarah Miller**\n",
      "\n",
      "* Experience:\n",
      "\t+ Analyzed market trends and customer behavior to inform marketing strategies (matches with \"Conduct research and support internal/external groups with the selection and implementation of applications and database management tools.\")\n",
      "\t+ Created interactive dashboards and visualizations to present data insights (matches with \"Design and automate routine and self-service reporting solutions (i.e., Power BI).\")\n",
      "* Education:\n",
      "\t+ Bachelor of Science in Statistics (relevant to the statistical analysis mentioned in the JD)\n",
      "\n",
      "**CV 2: Daniel Williams**\n",
      "\n",
      "* Experience:\n",
      "\t+ Designed and implemented scalable data pipelines on AWS (matches with \"Identify and utilize database management systems to aggregate and analyze data.\")\n",
      "\t+ Optimized ETL workflows for better performance (matches with \"Write scripts to support automation, data extraction, and reporting.\")\n",
      "\n",
      "**CV 3: Sophia Kim**\n",
      "\n",
      "* Experience:\n",
      "\t+ Developed machine learning models to predict customer churn (matches with \"Perform ad-hoc data extraction, analysis, and interpretation.\")\n",
      "\t+ Created visualizations to present data insights to stakeholders (matches with \"Design and automate routine and self-service reporting solutions (i.e., Power BI).\")\n",
      "* Education:\n",
      "\t+ Bachelor of Science in Statistics (relevant to the statistical analysis mentioned in the JD)\n",
      "* Certification:\n",
      "\t+ Certified Data Scientist (CDS) - Data Science Council of America (not directly relevant, but shows expertise in data science)\n",
      "\n",
      "No direct match for all responsibilities was found in any of the CVs. However, some experience and education may be relevant to certain responsibilities mentioned in the JD.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
